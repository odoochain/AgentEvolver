#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Tests for CachedSummarizedMemory and CacheRetrievalAgent."""

import sys
import os
import asyncio
import shutil
from pathlib import Path

# Ensure project root on path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from agentscope.message import Msg
from games.agent_factory import create_model_from_config
from games.agents.cache_retrieval_agent import CacheRetrievalAgent
from games.agents.memory import CachedSummarizedMemory


def _build_model_config() -> dict | None:
    """ä»ç¯å¢ƒå˜é‡æ„å»ºçœŸå®æ¨¡å‹é…ç½®ï¼Œç¼ºå¤±æ—¶è¿”å› None."""
    api_key = os.environ.get("OPENAI_API_KEY")
    base_url = os.environ.get("OPENAI_BASE_URL")
    model_name = os.environ.get("OPENAI_MODEL_NAME", "qwen-plus")
    if not api_key or not base_url:
        return None

    return {
        "model_name": model_name,
        "url": base_url,
        "api_key": api_key,
        "temperature": float(os.environ.get("OPENAI_TEMPERATURE", 0.2)),
        "max_tokens": int(os.environ.get("OPENAI_MAX_TOKENS", 256)),
        "stream": False,
    }


async def test_cached_memory_flush_and_load(tmp_dir: Path) -> None:
    model_cfg = _build_model_config()
    if model_cfg is None:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEY/OPENAI_BASE_URLï¼Œè·³è¿‡çœŸå®æ¨¡å‹æ‘˜è¦æµ‹è¯•")
        return

    # æ¨¡æ‹Ÿå¤–éƒ¨ä¼ å…¥ç¡®å®šçš„ log_dir
    game_log_dir = tmp_dir / "test_game_1"
    game_log_dir.mkdir(parents=True, exist_ok=True)

    memory = CachedSummarizedMemory(
        max_messages=5,
        log_dir=game_log_dir,  # æµ‹è¯•æ–°å‚æ•° log_dir
        game_id="test_game",
        memory_config=model_cfg,
    )

    if memory.summary_model is None:
        print("âš ï¸ æ‘˜è¦æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•")
        return

    # æ¨¡æ‹Ÿ 10 è½®å¯¹è¯ï¼Œå†åŠ  Moderator å°¾å·´ï¼Œç¡®ä¿è¶…è¿‡ max_messages=5 è§¦å‘ç¼“å­˜
    for i in range(10):
        role = "user" if i % 2 == 0 else "assistant"
        await memory.add(Msg(name=role, content=f"msg-{i}", role=role))
    await memory.add(Msg(name="Moderator", content="keep me", role="assistant"))

    merged = await memory.get_memory()
    print("ğŸ§ª flush å cache æ¡æ•°:", len(memory.cache))
    if len(memory.cache) == 0:
        print("âš ï¸ æœªç”Ÿæˆæ‘˜è¦ç¼“å­˜ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºæ‘˜è¦ï¼Œè·³è¿‡æ­¤ç”¨ä¾‹")
        return

    assert len(memory.content) == 1, "Content åº”åªä¿ç•™ Moderator å°¾å·´"
    assert merged[-1].name == "Moderator", "Moderator message preserved"

    # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦ç¬¦åˆé¢„æœŸï¼šlog_dir/cache/...
    expected_cache_dir = game_log_dir / "cache"
    assert expected_cache_dir.exists(), f"ç¼“å­˜ç›®å½•åº”åœ¨ {expected_cache_dir}"
    
    cached = await memory.load_cached_chunk(1)
    print("ğŸ§ª chunk#1 æ¡æ•°:", len(cached))
    assert len(cached) >= 5, "Cached chunk åº”åŒ…å«æº¢å‡ºçš„åŸå§‹æ¶ˆæ¯"


async def test_agent_recall_by_query(tmp_dir: Path) -> None:
    model_cfg = _build_model_config()
    if model_cfg is None:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEY/OPENAI_BASE_URLï¼Œè·³è¿‡æ£€ç´¢æ¨¡å‹æµ‹è¯•")
        return

    # æ¨¡æ‹Ÿå¤–éƒ¨ä¼ å…¥ç¡®å®šçš„ log_dir
    game_log_dir = tmp_dir / "test_game_agent"
    game_log_dir.mkdir(parents=True, exist_ok=True)

    memory = CachedSummarizedMemory(
        max_messages=5,
        log_dir=game_log_dir,  # æµ‹è¯•æ–°å‚æ•° log_dir
        game_id="test_game_agent",
        agent_id="tester",     # æµ‹è¯• agent_id ç»„åˆ
        memory_config=model_cfg,
    )
    
    # é¢„æœŸè·¯å¾„åº”è¯¥æ˜¯ log_dir/tester/cache
    # å› ä¸º CachedSummarizedMemory é€»è¾‘: if agent_id: cache_dir = base_path / str(agent_id) / "cache"
    expected_cache_dir = game_log_dir / "tester" / "cache"

    if memory.summary_model is None:
        print("âš ï¸ æ‘˜è¦æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡æ£€ç´¢æµ‹è¯•")
        return

    # æ¨¡æ‹Ÿ 10 è½®å¯¹è¯ï¼Œç¡®ä¿è§¦å‘ç¼“å­˜ï¼Œå†åŠ  Moderator å°¾å·´
    for i in range(10):
        role = "user" if i % 2 == 0 else "assistant"
        await memory.add(Msg(name=role, content=f"alpha-{i}", role=role))
    await memory.add(Msg(name="Moderator", content="tail", role="assistant"))
    await memory.get_memory()  # trigger flush to cache

    if len(memory.cache) == 0:
        print("âš ï¸ æœªç”Ÿæˆæ‘˜è¦ç¼“å­˜ï¼Œå¯èƒ½æ¨¡å‹è°ƒç”¨å¤±è´¥æˆ–è¿”å›ç©ºæ‘˜è¦ï¼Œè·³è¿‡æ£€ç´¢ç”¨ä¾‹")
        return
        
    assert expected_cache_dir.exists(), f"å¸¦AgentIDçš„ç¼“å­˜ç›®å½•åº”åœ¨ {expected_cache_dir}"

    main_model = create_model_from_config(model_cfg)
    retrieval_cfg = model_cfg.copy()

    agent = CacheRetrievalAgent(
        name="tester",
        sys_prompt="",
        model=main_model,
        formatter=None,
        memory=memory,
        retrieval_model_config=retrieval_cfg,
    )

    resp = await agent.recall_cache_by_query("alpha")
    print("ğŸ§ª æ£€ç´¢æ¨¡å‹è¾“å‡º:", resp.content)
    assert resp.metadata.get("cache_chunk_ids"), "æ£€ç´¢åº”è¿”å› chunk id åˆ—è¡¨"


def _make_tmp() -> Path:
    tmp_dir = Path(__file__).parent / "tmp_logs_test"
    if tmp_dir.exists():
        shutil.rmtree(tmp_dir)
    tmp_dir.mkdir(parents=True, exist_ok=True)
    return tmp_dir


async def main() -> None:
    tmp_dir = _make_tmp()
    try:
        await test_cached_memory_flush_and_load(tmp_dir)
        print("âœ“ CachedSummarizedMemory åˆ·ç›˜ä¸è¯»å–é€šè¿‡")
        await test_agent_recall_by_query(tmp_dir)
        print("âœ“ CacheRetrievalAgent æ£€ç´¢å¬å›é€šè¿‡")
        print("âœ… å…¨éƒ¨çœŸå®æ¨¡å‹ç¼“å­˜ç›¸å…³æµ‹è¯•é€šè¿‡")
    finally:
        shutil.rmtree(tmp_dir, ignore_errors=True)


if __name__ == "__main__":
    asyncio.run(main())

