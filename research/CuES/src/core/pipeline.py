"""
AgentFlow core pipeline
Coordinate the execution of Stage1, Stage2, and Stage3
"""
from typing import Dict, Any, List, Optional, Iterable
from concurrent.futures import ThreadPoolExecutor, as_completed
from ..core.api_client import DashScopeClient
from ..data.models import Triplet, Task, Session
from ..data.storage import DataStorage
from ..utils.logger import get_logger
from pathlib import Path

logger = get_logger(__name__)


class AgentFlowPipeline:
    """Main AgentFlow pipeline that coordinates the three stages"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # Initialize API client
        api_config = config.get('api', {})
        self.client = DashScopeClient(
            api_key=api_config.get('dashscope_api_key'),
            model_name=api_config.get('model_name', 'qwen-plus')
        )
        
        # Initialize data storage
        self.storage = DataStorage(config.get('data_dir', './data'))
        
        # Lazy init stages to avoid circular imports
        self._stage1 = None
        self._stage2 = None
        self._stage3 = None
    
    @property
    def stage1(self):
        """Lazily initialize Stage1"""
        if self._stage1 is None:
            from ..stages.stage1_triplet_generation import Stage1TripletGeneration
            stage1_config = self.config.get('stage1', {})
            max_steps = stage1_config.get('max_steps', 20)
            
            # Pass storage to Stage1
            self._stage1 = Stage1TripletGeneration(
                client=self.client,
                env_config=self.config.get('environment', {}),
                max_steps=max_steps,
                storage=self.storage,
                session_id=self.session_id
            )
        return self._stage1
    
    @property
    def stage2(self):
        """Lazily initialize Stage2"""
        if self._stage2 is None:
            from ..stages.stage2_task_abstraction import Stage2TaskAbstraction
            self._stage2 = Stage2TaskAbstraction(
                client=self.client,
                env_config=self.config.get('environment', {}),
                min_confidence=self.config.get('stage2', {}).get('min_confidence', 0.5),
                storage=self.storage  # New: pass storage
            )
        return self._stage2
    
    @property
    def stage3(self):
        """Lazily initialize Stage3"""
        if self._stage3 is None:
            stage3_config = self.config.get('stage3', {})
            if stage3_config:
                from ..stages.stage3_trajectory_generation import Stage3TrajectoryGeneration
                self._stage3 = Stage3TrajectoryGeneration(
                    client=self.client,
                    env_config=self.config.get('environment', {}),
                    data_dir=self.config.get('data_dir', './data'),
                    threading_config=self.config.get('threading', {}),
                    **stage3_config
                )
            else:
                self._stage3 = None
        return self._stage3


    def run_query_rewrite(
        self,
        *,
        trajectories_file: str,
        batch_size: int = 20,
        num_variants: int = 3,
        session_name: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Step 5. Query Rewrite
        - Input: a directory (containing trajectory_*.json generated by Stage 3) or a single trajectory_*.json file
        - Output: exactly the same JSON structure as Stage 3 outputs (per-file JSON, same keys, only replacing `query`)
        - Method: use messages context in each trajectory to rewrite the original query into semantically equivalent variants
        """

        import json
        import time
        from datetime import datetime

        start = time.time()
        in_path = Path(trajectories_file)
        if not in_path.exists():
            return {"success": False, "error": f"input path not found: {trajectories_file}"}

        # Collect trajectory files (successful ones), like trajectory_*.json
        files: List[Path] = []
        if in_path.is_dir():
            files = sorted([p for p in in_path.glob("trajectory_*.json") if p.is_file()], key=lambda p: p.stat().st_mtime)
        elif in_path.is_file() and in_path.suffix == ".json":
            files = [in_path]
        else:
            return {"success": False, "error": f"unsupported input: {trajectories_file}"}

        if not files:
            return {"success": False, "error": "no trajectory json files found"}

        # Output directory: per-file JSON
        out_dir = Path("./data/rewrites/trajectories")
        out_dir.mkdir(parents=True, exist_ok=True)

        rewritten = 0
        batch: List[Dict[str, Any]] = []
        batch_src: List[Path] = []

        def flush_batch():
            nonlocal rewritten
            if not batch:
                return
            try:
                rewritten += self._process_rewrite_batch_files(
                    batch_samples=batch,
                    batch_sources=batch_src,
                    num_variants=num_variants,
                    out_dir=out_dir
                )
            finally:
                batch.clear()
                batch_src.clear()

        for fp in files:
            try:
                with fp.open("r", encoding="utf-8") as f:
                    sample = json.load(f)
                batch.append(sample)
                batch_src.append(fp)
                if len(batch) >= batch_size:
                    flush_batch()
            except Exception as e:
                logger.warning(f"skip invalid trajectory file {fp.name}: {e}")

        # Flush the tail batch
        flush_batch()

        elapsed = time.time() - start
        return {
            "success": True,
            "rewritten_count": rewritten,
            "output_dir": str(out_dir),
            "seconds": elapsed,
            "session_name": session_name,
        }

    def _process_rewrite_batch_files(
        self,
        *,
        batch_samples: List[Dict[str, Any]],
        batch_sources: List["Path"],
        num_variants: int,
        out_dir: "Path",
    ) -> int:
        """Rewrite a batch of Stage 3 trajectory files. Clone and replace `query`, then write back JSON per file with the exact Stage 3 structure."""
        import json

        count = 0
        for sample, src in zip(batch_samples, batch_sources):
            # Read Stage 3 structure
            original_query = sample.get("query")
            messages = sample.get("messages", [])
            task_id = sample.get("task_id") or src.stem.replace("trajectory_", "")
            if not original_query or not isinstance(messages, list):
                # Skip invalid sample
                continue

            try:
                variants = self._rewrite_with_llm(
                    original_query=original_query,
                    messages=messages,
                    k=num_variants,
                )
            except Exception as e:
                logger.warning(f"rewrite failed for {src.name}: {e}")
                variants = []

            for i, v in enumerate(variants, start=1):
                # Clone and replace query; keep the exact key set as Stage 3
                new_obj = json.loads(json.dumps(sample, ensure_ascii=False))
                new_obj["query"] = v

                # File name: trajectory_<task_id>_rw{i}.json
                out_name = f"trajectory_{task_id}_rw{i}.json"
                out_fp = out_dir / out_name
                try:
                    with out_fp.open("w", encoding="utf-8") as wf:
                        json.dump(new_obj, wf, ensure_ascii=False, indent=2)
                    count += 1
                except Exception as werr:
                    logger.error(f"write rewrite file failed {out_name}: {werr}")
        return count

    def _rewrite_with_llm(self, *, original_query: str, messages: List[Dict[str, Any]], k: int) -> List[str]:
        """
        Use LLM to generate k semantically equivalent rewrites based on messages (trajectory context).
        Constraints: keep intent and difficulty consistent; avoid infeasible or oversimplified variants.
        """
        # Build a short context to control prompt length
        context = self._summarize_messages(messages)

        prompt = (
            "You are a precise query rewriting assistant. Given the original task query and a brief context summary from "
            "its trajectory, generate alternative phrasings that are semantically equivalent and executable in the same environment.\n"
            "Guidelines:\n"
            "- Control difficulty by how much of the context you implicitly encode: no or minimal hints => hardest; "
            "progressively adding concrete cues from the context => easier.\n"
            "- Maintain the original intent and constraints. Do not add or remove goals, entities, constraints, or assumptions. "
            "Avoid oversimplifying or making the task infeasible.\n"
            "- Keep rewrites diverse in wording/structure while preserving meaning. Keep each concise and similar in length to the original.\n"
            "- Use the same language as the original query. Do not repeat the original query verbatim.\n"
            "- If possible, order from harder to easier (top to bottom).\n"
            "Output format:\n"
            f"- Produce at most {k} rewrites, one per line, with no numbering, bullets, or explanations.\n"
            "Inputs:\n"
            f"- Original query: {original_query}\n"
            f"- Context summary: {context}\n"
            "Now provide the rewrites:"
        )
        messages_payload = [
            {"role": "system", "content": "You are a rigorous query rewriting assistant."},
            {"role": "user", "content": prompt},
        ]

        try:
            text = self.client.chat_with_retry(messages_payload, max_retries=2)
        except Exception:
            # Fallback: simple template to keep the pipeline running
            return [f"{original_query} (rewrite {i})" for i in range(1, k + 1)]

        # Parse lines
        lines = [ln.strip() for ln in str(text).splitlines()]
        cleaned: List[str] = []
        for ln in lines:
            s = ln.lstrip("-•").lstrip()
            # Strip numbered prefixes
            while s and (s[0].isdigit() or s[0] in "、."):
                s = s[1:].lstrip()
            if s and s not in cleaned and s != original_query:
                cleaned.append(s)
            if len(cleaned) >= k:
                break

        # Pad if insufficient
        while len(cleaned) < k:
            cand = f"{original_query} (rewrite {len(cleaned)+1})"
            if cand not in cleaned:
                cleaned.append(cand)
            else:
                break
        return cleaned[:k]

    def _summarize_messages(self, messages: List[Dict[str, Any]]) -> str:
        """Summarize Stage 3 messages into a short context string"""
        try:
            if isinstance(messages, list):
                # Take the most recent rounds and concatenate
                excerpts: List[str] = []
                for m in messages[-8:]:
                    c = str(m.get("content", "")).strip()
                    if c:
                        excerpts.append(c)
                summary = " | ".join(excerpts)
                return summary[:500]
        except Exception:
            pass
        return "N/A"

    def extract_concept(self, batch_size: int = 10, max_workers: int = 10, query_count: int = 100) -> List[str]:
        """Use multithreading to get the query for each task and batch them into LLM to extract nouns"""
        from EnvService.env_sandbox.env_client import EnvClient
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import ast

        llmclient = self.client

        # Read EnvService settings from config instead of hardcoding
        env_cfg = self.config.get('environment', {}).get('envservice', {})
        server_url = env_cfg.get('server_url', 'http://localhost:8080')
        env_type = env_cfg.get('env_type', self.config.get('environment', {}).get('type', 'webshop'))
        create_params = env_cfg.get('create_params', {})  # optional extra params

        envclient = EnvClient(server_url)

        # Get task IDs and shuffle
        task_ids = envclient.get_env_profile(env_type, split='train')
        import random
        random.shuffle(task_ids)

        # Use multithreading to get the query for each task
        def get_query(task_id):
            try:
                init_response = envclient.create_instance(
                    env_type,
                    task_id,
                    params=create_params,
                )
                instance_id = init_response["info"]["instance_id"]
                query = (
                    init_response["state"][-1]
                    .get('content', '')
                    .replace('WebShop\nInstruction: \n', '')
                    .strip()
                )
                envclient.release_instance(instance_id)
                return str(query)
            except Exception as e:
                logger.warning(f"extract_concept: failed to fetch query for task {task_id}: {e}")
                return ""

        queries = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {executor.submit(get_query, tid): tid for tid in task_ids[:query_count]}
            for future in as_completed(future_to_id):
                try:
                    query = future.result()
                    if query:
                        queries.append(query)
                except Exception as e:
                    logger.warning(f"extract_concept: future failed: {e}")

        # Batch them into LLM by batch_size
        batches = [queries[i:i + batch_size] for i in range(0, len(queries), batch_size)]

        def process_batch(batch_queries):
            prompt = (
                "Please extract all nouns (concepts) involved from the following queries. Only return a list of nouns, do not explain:\n"
                + "\n".join(batch_queries)
            )
            messages = [
                {"role": "system", "content": "You are a professional noun extraction assistant."},
                {"role": "user", "content": prompt}
            ]
            response = llmclient.chat_with_retry(messages, max_retries=2)
            try:
                concepts = ast.literal_eval(response.strip())
                if not isinstance(concepts, list):
                    concepts = [concepts]
            except Exception:
                # Support comma or newline separation
                import re
                concepts = [x.strip() for x in re.split(r'[\,\n]', response) if x.strip()]
            return concepts

        all_concepts = set()
        for batch in batches:
            concepts = process_batch(batch)
            all_concepts.update(concepts)

        return list(all_concepts)
    
    def run_full_pipeline(self, session_name: Optional[str] = None, requirement: Optional[str] = None, concepts: Optional[list] = None) -> Dict[str, Any]:
        """Run the full three-stage pipeline, with optional exploration requirements"""

        logger.info("Starting full AgentFlow pipeline execution")
        if concepts:
            logger.info(f"Using concept set: {concepts}")
        
        if requirement:
            logger.info(f"User-specified exploration requirement: {requirement}")
        
        self.requirement = requirement
        self.concepts = concepts
        
        # Create session
        session = Session()
        self.storage.save_session(session)
        session_id = session.session_id
        logger.info(f"Created session: {session_id}")
        
        # try:
        # Stage 1: Generate triplets
        print("=== Stage 1: Triplet Generation ===")
        stage1_config = self.config.get('stage1', {})
        rollout_num = stage1_config.get('rollout_num', 3)

        triplets = self._run_stage1_parallel(rollout_num=rollout_num, requirement=requirement, concepts=concepts, session_id=session_id)

        if not triplets:
            logger.error("Stage 1 did not generate any triplets, stopping execution")
            return {'success': False, 'error': 'No triplets generated'}
        
        logger.info(f"Stage 1 completed, generated {len(triplets)} triplets")
        
        # Stage 2: Task abstraction
        print("=== Stage 2: Task Abstraction ===")
        stage2_config = self.config.get('stage2', {})
        batch_size = stage2_config.get('batch_size', 10)
        
        tasks = self._run_stage2_parallel(triplets, batch_size, session_id)
        stage2_tasks_count = len(tasks) if tasks else 0
        
        if not tasks:
            logger.warning("Stage 2 did not abstract any tasks")
        else:
            # # Save tasks
            # for task in tasks:
            #     task.session_id = session_id
            #     self.storage.save_task(task)
            
            stage2_tasks_count = len(tasks)
            print(f"Stage 2 completed, abstracted {stage2_tasks_count} tasks")
        
        # Stage 3: Trajectory generation (if enabled)
        stage3_results = None
        if self.stage3 and tasks:
            print("=== Stage 3: Trajectory Generation ===")
            stage3_results = self._run_stage3_parallel([task.dict() if hasattr(task, 'dict') else task.model_dump() for task in tasks])
        
        # Generate statistics
        stats = self._generate_statistics(triplets, tasks)
        if stage3_results:
            stats.update(stage3_results.get('statistics', {}))
        
        print("AgentFlow Core pipeline execution completed")
        
        result = {
            'success': True,
            'session_id': session_id,
            'triplets_count': len(triplets),
            'tasks_count': stage2_tasks_count,
            'trajectories_count': stage3_results.get('statistics', {}).get('successful', 0) if stage3_results else 0,
            'statistics': stats
        }
        
        if stage3_results:
            result['stage3_results'] = stage3_results
        
        return result
            
        # except Exception as e:
        #     logger.error(f"Pipeline execution failed: {e}")
        #     return {'success': False, 'error': str(e)}
            
    
    def run_stage1_only(self, requirement: Optional[str] = None, concepts: Optional[list] = None) -> Dict[str, Any]:
        """Run only Stage 1, with optional exploration requirements"""
        logger.info("Running only Stage 1: Triplet Generation")
        if requirement:
            logger.info(f"User-specified exploration requirement: {requirement}")

        self.requirement = requirement
        self.concepts = concepts
        
        # Create session
        session = Session()
        self.storage.save_session(session)
        session_id = session.session_id
        logger.info(f"Created session: {session_id}")
        
        try:
            # Stage 1: Generate triplets
            print("=== Stage 1: Triplet Generation ===")
            stage1_config = self.config.get('stage1', {})
            rollout_num = stage1_config.get('rollout_num', 3)

            triplets = self._run_stage1_parallel(rollout_num=rollout_num, requirement=requirement, concepts=concepts, session_id=session_id)

            if not triplets:
                logger.error("Stage 1 did not generate any triplets")
                return {'success': False, 'error': 'No triplets generated'}
            
            logger.info(f"Stage 1 completed, generated {len(triplets)} triplets")
            
            # Generate statistics
            stats = self._generate_statistics(triplets, [])
            
            print("Stage 1 execution completed")
            
            return {
                'success': True,
                'session_id': session_id,
                'triplets_count': len(triplets),
                'tasks_count': 0,
                'trajectories_count': 0,
                'statistics': stats
            }
            
        except Exception as e:
            logger.error(f"Stage 1 execution failed: {e}")
            return {'success': False, 'error': str(e)}

    def run_stage2_only(self, triplets_file: str) -> Dict[str, Any]:
        """Run only Stage 2, reading triplets from a jsonl file"""
        logger.info("Running only Stage 2: Task Abstraction")
        logger.info(f"Reading triplets from file: {triplets_file}")
        
        # Create session
        session = Session()
        self.storage.save_session(session)
        session_id = session.session_id
        logger.info(f"Created session: {session_id}")
        
        try:
            # Read triplets file
            import jsonlines
            from pathlib import Path
            from datetime import datetime
            
            triplets = []
            triplets_path = Path(triplets_file)
            
            if not triplets_path.exists():
                logger.error(f"Triplets file not found: {triplets_file}")
                return {'success': False, 'error': f'Triplets file not found: {triplets_file}'}
            
            with jsonlines.open(triplets_path, mode='r') as reader:
                for obj in reader:
                    # Handle datetime fields
                    if 'timestamp' in obj and isinstance(obj['timestamp'], str):
                        obj['timestamp'] = datetime.fromisoformat(obj['timestamp'])
                    triplets.append(Triplet(**obj))
            
            logger.info(f"Read {len(triplets)} triplets from file")
            
            if not triplets:
                logger.error("No triplets found in file")
                return {'success': False, 'error': 'No triplets found in file'}
            
            # Stage 2: Task abstraction
            print("=== Stage 2: Task Abstraction ===")
            stage2_config = self.config.get('stage2', {})
            batch_size = stage2_config.get('batch_size', 10)
            
            tasks = self._run_stage2_parallel(triplets, batch_size, session_id)
            stage2_tasks_count = len(tasks) if tasks else 0
            
            if not tasks:
                logger.warning("Stage 2 did not abstract any tasks")
            else:
                stage2_tasks_count = len(tasks)
                print(f"Stage 2 completed, abstracted {stage2_tasks_count} tasks")
            
            # Generate statistics
            stats = self._generate_statistics(triplets, tasks)
            
            print("Stage 2 execution completed")
            
            return {
                'success': True,
                'session_id': session_id,
                'triplets_count': len(triplets),
                'tasks_count': stage2_tasks_count,
                'trajectories_count': 0,
                'statistics': stats
            }
            
        except Exception as e:
            logger.error(f"Stage 2 execution failed: {e}")
            return {'success': False, 'error': str(e)}

    def run_stage3_only(self, tasks_file: str) -> Dict[str, Any]:
        """Run only Stage 3, reading tasks from a jsonl file"""
        if not self.stage3:
            logger.error("Stage3 not initialized. Please add stage3 configuration.")
            return {'success': False, 'error': 'Stage3 not configured'}
        
        logger.info("Running only Stage 3: Trajectory Generation")
        logger.info(f"Reading tasks from file: {tasks_file}")
        
        # Create session
        session = Session()
        self.storage.save_session(session)
        session_id = session.session_id
        logger.info(f"Created session: {session_id}")
        
        try:
            # Read tasks file
            import jsonlines
            from pathlib import Path
            from datetime import datetime
            
            tasks = []
            tasks_path = Path(tasks_file)
            
            if not tasks_path.exists():
                logger.error(f"Tasks file not found: {tasks_file}")
                return {'success': False, 'error': f'Tasks file not found: {tasks_file}'}
            
            # Check file extension, support json and jsonl
            if tasks_path.suffix == '.json':
                import json
                with open(tasks_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for task_data in data:
                            # Handle datetime fields
                            if 'timestamp' in task_data and isinstance(task_data['timestamp'], str):
                                task_data['timestamp'] = datetime.fromisoformat(task_data['timestamp'])
                            tasks.append(task_data)
                    else:
                        # Handle datetime fields
                        if 'timestamp' in data and isinstance(data['timestamp'], str):
                            data['timestamp'] = datetime.fromisoformat(data['timestamp'])
                        tasks.append(data)
            else:
                # Default to jsonl
                with jsonlines.open(tasks_path, mode='r') as reader:
                    for obj in reader:
                        # Handle datetime fields
                        if 'timestamp' in obj and isinstance(obj['timestamp'], str):
                            obj['timestamp'] = datetime.fromisoformat(obj['timestamp'])
                        tasks.append(obj)
            
            logger.info(f"Read {len(tasks)} tasks from file")
            
            if not tasks:
                logger.error("No tasks found in file")
                return {'success': False, 'error': 'No tasks found in file'}
            
            # Stage 3: Trajectory generation
            print("=== Stage 3: Trajectory Generation ===")
            stage3_results = self._run_stage3_parallel(tasks)
            
            # Generate statistics
            stats = stage3_results.get('statistics', {})
            
            print("Stage 3 execution completed")
            
            return {
                'success': True,
                'session_id': session_id,
                'triplets_count': 0,
                'tasks_count': len(tasks),
                'trajectories_count': stage3_results.get('statistics', {}).get('successful', 0),
                'statistics': {'trajectories': stats}
            }
            
        except Exception as e:
            logger.error(f"Stage 3 execution failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_statistics(self, triplets: List[Triplet], tasks: List[Task]) -> Dict[str, Any]:
        """Generate statistics"""
        triplet_stats = {
            'total_triplets': len(triplets),
            'avg_reward': sum(t.reward for t in triplets) / len(triplets) if triplets else 0,
            'success_rate': len([t for t in triplets if t.done]) / len(triplets) if triplets else 0
        }
        
        task_stats = self.stage2.get_task_statistics(tasks)
        
        return {
            'triplets': triplet_stats,
            'tasks': task_stats
        }
    
    def get_session_results(self, session_id: str) -> Dict[str, Any]:
        """Get session results"""
        try:
            session = self.storage.load_session(session_id)
            triplets = self.storage.load_triplets_by_session(session_id)
            tasks = self.storage.load_tasks_by_session(session_id)
            
            return {
                'session': session.to_dict() if session else None,
                'triplets': [t.to_dict() for t in triplets],
                'tasks': [t.to_dict() for t in tasks],
                'statistics': self._generate_statistics(triplets, tasks)
            }
            
        except Exception as e:
            logger.error(f"Failed to get session results: {e}")
            return {'error': str(e)}
    
    def _run_stage1_parallel(self, rollout_num: int, requirement: Optional[str] = None, session_id: Optional[str] = None, concepts: Optional[list] = None) -> List:
        """Run Stage1 rollouts in parallel using multithreading"""
        # Get threading config
        threading_config = self.config.get('threading', {})
        max_workers = threading_config.get('max_workers', 10)
        enabled = threading_config.get('enabled', True)
        
        # If multithreading is disabled or rollout number is small, run serially
        if not enabled or rollout_num <= 1 or max_workers <= 1:
            logger.info("Stage1 using serial execution mode")
            return self.stage1.run(rollout_num=rollout_num, requirement=requirement, concepts=concepts)
        
        print(f"Stage1 using multithreading - workers: {max_workers}, rollouts: {rollout_num}")
        
        # Create a separate Stage1 instance for each thread
        def create_stage1_instance():
            from ..stages.stage1_triplet_generation import Stage1TripletGeneration
            return Stage1TripletGeneration(
                client=self.client,
                env_config=self.config.get('environment', {}),
                max_steps=self.config.get('stage1', {}).get('max_steps', 20),
                storage=self.storage, # Pass storage to support memory
                session_id=session_id
            )
        
        def run_single_rollout(rollout_idx):
            """Function to execute a single rollout"""
            # try:
            # Create a separate Stage1 instance for each rollout to avoid thread conflicts
            stage1_instance = create_stage1_instance()
            # Set exploration requirement
            if self.requirement or self.concepts:
                stage1_instance.set_exploration_requirement(requirement=self.requirement,concepts=self.concepts)
            triplets = stage1_instance._single_rollout()
            logger.info(f"Rollout {rollout_idx + 1} completed, generated {len(triplets)} triplets")
            return triplets
            # except Exception as e:
            #     logger.error(f"Rollout {rollout_idx + 1} failed: {e}")
            #     return []
        
        # Execute using thread pool
        all_triplets = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all rollout tasks
            future_to_idx = {executor.submit(run_single_rollout, i): i for i in range(rollout_num)}
            
            # Collect results
            completed_count = 0
            for future in as_completed(future_to_idx):
                completed_count += 1
                rollout_idx = future_to_idx[future]
                triplets = future.result()
                all_triplets.extend(triplets)
                logger.info(f"Rollout {rollout_idx + 1} collected ({completed_count}/{rollout_num})")
        
        logger.info(f"Multithreaded Stage1 completed: generated {len(all_triplets)} triplets")
        return all_triplets
    
    def _run_stage2_parallel(self, triplets: List, batch_size: int, session_id: Optional[str] = None) -> List:
        """Run Stage2 batch processing in parallel using multithreading"""
        # Get threading config
        threading_config = self.config.get('threading', {})
        max_workers = threading_config.get('max_workers', 10)
        enabled = threading_config.get('enabled', True)
        
        if not triplets:
            logger.warning("No triplets data for task abstraction")
            return []
        
        # Group triplets by env_id (copy Stage2 logic)
        env_groups = {}
        for triplet in triplets:
            env_id = triplet.env_id
            if env_id not in env_groups:
                env_groups[env_id] = []
            env_groups[env_id].append(triplet)
        
        # Create batch list
        batch_list = []
        batch_num = 1
        for env_id, env_triplets in env_groups.items():
            for i in range(0, len(env_triplets), batch_size):
                batch_triplets = env_triplets[i:i + batch_size]
                batch_list.append((batch_triplets, batch_num, env_id))
                batch_num += 1
        
        # If multithreading is disabled or batch number is small, run serially
        if not enabled or len(batch_list) <= 1 or max_workers <= 1:
            logger.info("Stage2 using serial execution mode")
            return self.stage2.run(triplets, batch_size=batch_size)
        
        logger.info(f"Stage2 using multithreading - workers: {max_workers}, batches: {len(batch_list)}")
        
        # Create a separate Stage2 instance for each thread
        def create_stage2_instance():
            from ..stages.stage2_task_abstraction import Stage2TaskAbstraction
            return Stage2TaskAbstraction(
                client=self.client,
                env_config=self.config.get('environment', {}),
                min_confidence=self.config.get('stage2', {}).get('min_confidence', 0.5),
                storage=self.storage,  # New: pass storage
                session_id=session_id
            )
        
        def process_single_batch(batch_data):
            """Function to process a single batch"""
            batch_triplets, batch_num, env_id = batch_data
            try:
                # Create a separate Stage2 instance for each batch to avoid thread conflicts
                stage2_instance = create_stage2_instance()
                tasks = stage2_instance._extract_tasks_from_batch(batch_triplets, batch_num, env_id)
                logger.info(f"Batch {batch_num} completed, abstracted {len(tasks)} tasks")
                return tasks
            except Exception as e:
                logger.error(f"Batch {batch_num} failed: {e}")
                return []
        
        # Execute using thread pool
        all_tasks = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all batch tasks
            future_to_batch = {executor.submit(process_single_batch, batch_data): batch_data for batch_data in batch_list}
            
            # Collect results
            completed_count = 0
            for future in as_completed(future_to_batch):
                completed_count += 1
                batch_data = future_to_batch[future]
                batch_num = batch_data[1]
                tasks = future.result()
                all_tasks.extend(tasks)
                logger.info(f"Batch {batch_num} collected ({completed_count}/{len(batch_list)})")
        
        # Deduplicate and filter (copy Stage2 logic)
        stage2_instance = create_stage2_instance()
        filtered_tasks = stage2_instance._filter_and_deduplicate_tasks(all_tasks)
        
        logger.info(f"Multithreaded Stage2 completed: abstracted {len(filtered_tasks)} tasks")
        return filtered_tasks
    
    def _run_stage3_parallel(self, task_dicts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Run Stage3 task processing in parallel using multithreading"""
        # Get threading config
        threading_config = self.config.get('threading', {})
        max_workers = threading_config.get('max_workers', 10)
        enabled = threading_config.get('enabled', True)
        
        # If multithreading is disabled or task number is small, run serially
        if not enabled or len(task_dicts) <= 1 or max_workers <= 1:
            logger.info("Using serial execution mode")
            return self.stage3.run(task_dicts)
        
        logger.info(f"Using multithreading - workers: {max_workers}, tasks: {len(task_dicts)}")
        
        # Initialize result statistics
        results = {
            "successful_trajectories": [],
            "failed_tasks": [],
            "statistics": {
                "total_tasks": len(task_dicts),
                "successful": 0,
                "failed": 0,
                "strategy1_success": 0,
                "strategy2_success": 0
            }
        }
        
        # Create a separate Stage3 instance for each thread
        def create_stage3_instance():
            from ..stages.stage3_trajectory_generation import Stage3TrajectoryGeneration
            stage3_config = self.config.get('stage3', {})
            return Stage3TrajectoryGeneration(
                client=self.client,
                env_config=self.config.get('environment', {}),
                data_dir=self.config.get('data_dir', './data'),
                **stage3_config
            )
        
        def process_single_task(task_dict):
            """Function to process a single task"""
            try:
                # Create a separate Stage3 instance for each task to avoid thread conflicts
                stage3_instance = create_stage3_instance()
                task_result = stage3_instance._generate_single_trajectory(
                    task_dict, task_dict.get('env_id', 'unknown_env')
                )
                return task_dict, task_result, None
            except Exception as e:
                logger.error(f"Failed to process task {task_dict.get('task_id', 'unknown')}: {e}")
                return task_dict, None, str(e)
        
        # Execute using thread pool
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all task tasks
            future_to_task = {executor.submit(process_single_task, task): task for task in task_dicts}
            
            # Collect results
            completed_count = 0
            for future in as_completed(future_to_task):
                completed_count += 1
                task_dict, trajectory, error = future.result()
                task_id = task_dict.get('task_id', 'unknown')
                
                if error:
                    results["failed_tasks"].append(task_dict)
                    results["statistics"]["failed"] += 1
                    # Save failed task metadata instead of trying to save a non-existent trajectory
                    stage3_instance = create_stage3_instance()
                    stage3_instance._save_failed_task(task_dict, f"exception: {error}")
                elif trajectory and trajectory.success:
                    results["successful_trajectories"].append(trajectory)
                    results["statistics"]["successful"] += 1
                    
                    if trajectory.strategy_used == "simple":
                        results["statistics"]["strategy1_success"] += 1
                    else:
                        results["statistics"]["strategy2_success"] += 1
                    
                    # Save successful trajectory
                    stage3_instance = create_stage3_instance()
                    stage3_instance._save_trajectory(trajectory)
                    logger.info(f"Task {task_id} completed ({completed_count}/{len(task_dicts)})")
                else:
                    results["failed_tasks"].append(task_dict)
                    results["statistics"]["failed"] += 1
                    # Save failed task
                    stage3_instance = create_stage3_instance()
                    # stage3_instance._save_failed_task(task_dict, "execution_failed")
                    if trajectory:
                        # If there is a trajectory but execution failed, save as failed trajectory
                        stage3_instance._save_trajectory(trajectory, failed=True)
                    else:
                        stage3_instance._save_failed_task(task_dict, "execution_failed")
                    logger.warning(f"Task {task_id} execution failed ({completed_count}/{len(task_dicts)})")
        
        # Save statistics
        stage3_instance = create_stage3_instance()
        stage3_instance._save_statistics(results["statistics"])
        
        success_rate = results['statistics']['successful'] / results['statistics']['total_tasks'] if results['statistics']['total_tasks'] > 0 else 0
        logger.info(f"Multithreaded Stage3 completed: {results['statistics']['successful']}/{results['statistics']['total_tasks']} successful ({success_rate:.2%})")
        
        return results
